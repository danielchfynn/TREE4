24th June 2023

Cart – includes cart.method = “LATENT-BUDGET-TREE” 

The creates a tree using latent budgets, utilising the rpy2 library and lba function in R. 
Assessing returned alpha values greater than 0.5 and assigning the corresponding class to the left split. 

Please note for the creation of the class element, an impurity function is still passed, 
such as gini, and is used during display only. 

Also latent budget tree is only available for classifier problems

This takes the max_k (complete) top tau predictors to assess. Complete as in no error is returned from 
the lba function: such as inversing non square matricies. This max_k parameter can be passed to the cart.growing_tree() function.

I am confident in the tau calculations comparing with R GKtau library. 

Also the cart.print_tree() function has been updated to include the percentages of all the classes in the nodes, 
to see how it goes. The cart.print_tree() also takes hyperparameter table = True to return a pandas dataframe of 
the information about the tree. This includes the alpha and beta parameters from the split but is a bit messy. 

Please note this table is only available for latent budget tree at this time. 

Also the print_tree() function takes html as a parameter that will open the graph in your browser and create a html
 object in your directory. This was because in jupyter the graphs could be a bit small and hard to distinguish, 
especially with increasing depth. 

A quite weird thing, is as it is a classifier problem all observations are strings / objects for the predictors 
and that if using pandas, for importing the data frame both training and test tests will need to have index’s starting at 0

I have included my play dataset, which is Christians, and is the transport dataset and is attached. 





18/7/23

combination_split
included combination_split paramater in growing_tree - which will only work for lbt 

This simple combines the groups for the top two tau. - and also works with max_k, so will iterate towards the the 2nd and 3rd highest ect. 

FOr example distance and car_ownership make a new group distance*car_ownership
where the classes (strings) are concatenated such as 
car_ownership0 (a class in car ownershuip) + distance02
would become car_onership0distance02.

The prints are very busy, when there are a combination of many classes 

I add the best split found from this to the list in cart, n_features, which can be accessed after class creation, for inspectoin. 


Performed fixed for the node proportion total and gain, to make correct use of delta (the change in done deviance explained) as before it was cheated a bit. 

At the moment, for regression it is calculated with sum {i node in leaves} (n|i* mean(y|i) -mean(y)**2)
where n|i is the number in the ith node, and y|i are the y values in the ith node

And for classification is the sum of the entropy in the leaves 

Printing trees with table is available for all methods 



Twoing
Started to apply twoing algorithm
Create eevry combination of the response variable 
Reassign the y value for that 
####AM using gini / entropy as the splitting choice metric for lbt instead of outputed ls
also am using the actual gini/entropy in the results table for the j classes, not the c1,c2, classes
Notice when running lbt using k = 1 is ok, as there are many errors from lba function when running this
Min imp gain set at 1e-6, max level = 4, min class parent 1000, min class child 500 
Also implemented table visuals 

ALso implemented table visuals for use with cart, fast and twostage, all are considerably slower than lba.


Twoing Regression
Taking the set of each value in the y values in the node (starting with root)
iterate through each value in the set, and calculate the between variance of the groups, 
c1 is for each value < the max between variance, c2 otherwise
change the cart setting into classifier settings with appropiate impur_fn
send this all to the node_search_split and apply required method either cart, fast, two-stage /lbt 
	have only ran fast so far 
	also works cart
	also for lbt
iterate this process

created image to show how the code works for twoing for both class and reg



attempts made with visual pruning but not fruitful. Attempting first to have the same lebgths, but continually had crossing branches. May have to implement a plotting algorithm for this. 

we can use node prop gains for the length of the branches (in some way) 

26/7/23
Have done a bit more tweaking to the binpi methods, found some inconsistencies. Have also a new run sheet using the cdc heart disease data set bimi_applied_heart. performed comparison between cart libary bimpi imputation with fast, and using sklearn adaboost function. 

9/9/23
Upload of updated cart, includes minor functional changes, inclusing a now working combination_split possibility to lbt(). Also a major correction in the prediction function, with the misordering of the dictionary and for loop there. 
pruning also works for all methods (except maybe adaboost on its own, which hasnt been tested for individual use, and only within binpi)

uploaded other working sheets like teh heart dataset preprocessing
heart_disease_pre_imp.csv for use in the binpi_heart set, as a zip
full_data_heart.csv for use with the preprocessing sheet to creaste the heart_disease_pre_imp.csv - can't upload because of the size of the file, but there are instructions about how to get the data within the preprocessing worksheet

uploaded prova_twoing_regression.ipynb - but this can also be implemented from prova_regressionjupyter.ipynb

also the comparison sheets have been uploaded in comparisons folder for scikit, r, and matlab



20/2/24

Have reworked parts of CARTplus to allow for the use of ndarrays as input. this makes it run quite a bit faster

attached will be two example worksheets showing this method

Also i have updated the best_split for regressions to take the midway points between consecutive ordered values, as done in tree and matlab fitrtree

Also i have made a possible adimission for classification trees, where if the class of two children from one parents have the same value, this will prune. This can be passed in the print_tree and prunign option, under a boolean merge_leaves. This is the same as matlab fitctree (this will work for regreesion trees if two branches have exactly the same values, so unlikely to affect) 
This has been trialed with Carseats_train dataset, and gives the same result as the pruned tree, so havent seen how to deviate this yet. 

Visual pruning is in place. Have some questions about it, but it looks ok. Has an option within the program to be upward growing or downward growin, have left it downward, but would be an easy parameter to make if needed. 

At the moment using the node proportion gain to define the change of length in the y plane. The node proportion gain is the difference between including the split and not. 


the node proportion total is defined for a classifier as:

gini value of the initial y - \sum_L [1 - \sum_C(n_C / n_L)^2]*(n_L/n)

L is for leaves C is for classes. 


for regression is the:

\sum_L n_L (\mu_L - \hat{y})^2 / pvariance (y) 






- ALso added the deviance value for the final table, and can see the values are the same as tree library. I found a deviance formula from teh MASS textbook - very similar to entropy 

D_i = -2 * sum_k (n_ik log p_ik) 

the difference with entropy is it is natural log not log 2, and the coefficient




Added a new class named k-folds to the CARTplus library. Esentially takes as input the same values as CART, plus a k value for the amoutn of folds. At the moment it just print the errors for each model, as well as the average error. and it stores the models if wanted for later use. 


All these sections have been updated on the Overleaf for instructions


1/3/24
Updating cartplus to treeplus and other small changes 

2/3/23

returned stump from node search split

including surrogate split

surrogate split is chosen by investigating each split that is made when searching for the split (ignoring the splits from the same variable). and comparing the agreement with both left and right from the best split with the left and right node of the proposed surrogate, finally dividing the count of agreements by the number observations that went left and right for the best split. 


leftind = Counter(beststump[0].indexes) 
rightind = Counter(beststump[1].indexes) 

for i in self.nss_stumps: #goes through each stump
    if i != bestvar:      #makes sure its not teh same var 
                
	#finding the overlap  divided by total obs in best split 
	leftmatch =len(set(leftind). intersection(Counter(i[0].indexes))) / len(beststump[0].indexes)
                
	rightmatch = len(set(rightind). intersection(Counter(i[1].indexes))) / len(beststump[1].indexes)

at the moment i check if right match and left match > 0.65 to be added to the node

This can be viewed by:

for i in tree.get_all_node():
    print(i.name)
    for j in i.surrogate_splits:
        print(j)

but at the moemnt aren't used in any capacity. 

0.65 was chosen by experimenting with different values. 

has been test for all methds 


16/3/24

added categorise_num: for two-stage regressionly only
default is supervised aka retain y when splitting 
splitting rule on max_level = 2, 
idea was to use internode variance - but waiting for advise on this
only implemented at start of tree growing, with flexibility to expand to each father node
timign takes a little bit , for the most part implemented with numpy
can see eta^2 is nto close to 1 for num vars now. 

5/6/24
Pypi uploads required a change in name of the library from TREEplus to TREE4. 
TREE4 can be pip installed 
And teh objects in the library activated by TREE4.TREE4 import *
Also uploaded the reference manual 
